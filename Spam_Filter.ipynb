{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_k62KJtCfagW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "root_download = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = root_download + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = root_download + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nctz31o4aBWe"
   },
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2oK16QCbanLU"
   },
   "outputs": [],
   "source": [
    "easy_ham_dir = os.path.join(SPAM_PATH, 'easy_ham')\n",
    "spam_dir = os.path.join(SPAM_PATH, 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FVYRsLNcYGGo"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lF4YncIldzwO"
   },
   "outputs": [],
   "source": [
    "#deleting the last element - 'cmds'.\n",
    "ham_names = np.array(sorted(os.listdir(easy_ham_dir)))[:-1]\n",
    "spam_names = np.array(sorted(os.listdir(spam_dir)))[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YfU5X3D-ePtK"
   },
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aC16Ebi8gX4I"
   },
   "outputs": [],
   "source": [
    "def get_email(category, filename):\n",
    "  directory = easy_ham_dir if category == 'ham' else spam_dir\n",
    "  with open(os.path.join(directory, filename), 'rb') as f:\n",
    "    return email.parser.BytesParser(policy = email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7tEUDQEthWdr"
   },
   "outputs": [],
   "source": [
    "hams = [get_email('ham', name) for name in ham_names]\n",
    "spams = [get_email('spam', name) for name in spam_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering the types of emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q4v-hAO-nEbG"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_uOykMXuF5_",
    "outputId": "bee8f411-892a-4875-8fd0-a188f027d41c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('text/plain', 2408), ('multipart/signed', 68), ('multipart/alternative', 9), ('multipart/mixed', 10), ('multipart/related', 3), ('multipart/report', 2)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TYPES = [email.get_content_type() for email in (hams)]\n",
    "c = Counter(TYPES)\n",
    "c.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nuSQVUfvyC1",
    "outputId": "ae49801c-9c50-4b7f-971a-e31e4b9704ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('text/html', 183), ('text/plain', 218), ('multipart/mixed', 43), ('multipart/alternative', 47), ('multipart/related', 9)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TYPES = [email.get_content_type() for email in (spams)]\n",
    "c = Counter(TYPES)\n",
    "c.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OKube3j8Tvc",
    "outputId": "8a895f8f-8dad-4a5f-9780-1bb6d2f964f8"
   },
   "outputs": [],
   "source": [
    "X = np.array(hams + spams, dtype = 'object')\n",
    "y = np.array([1] * len(hams) + [0] * len(spams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "1iARsGCO-UwB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "zyPiR57--gMz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scgnyWYK-13S"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_plain_text_test(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.replace_with(\" HYPERLINK \")\n",
    "    return soup.get_text(separator = \" \", strip = True)\n",
    "\n",
    "def email_to_text_test(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text_test(html)\n",
    "    \n",
    "def refine_test(email_text):\n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    urls = url_extractor.find_urls(email_text)\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    email_text = re.sub(email_pattern, \"email_address\", email_text)\n",
    "    for url in urls:\n",
    "        email_text = re.sub(re.escape(url), 'url', email_text)\n",
    "    email_text = re.sub(r'\\d+', '', email_text)\n",
    "    email_text = re.sub(r'[^\\w\\s]', '', email_text)\n",
    "    return email_text\n",
    "\n",
    "def preprocess_test(email_parsed):\n",
    "    email_text = email_to_text_test(email_parsed)\n",
    "    email_text = refine_test(email_text)\n",
    "    cnt = Counter()\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in email_text.split():\n",
    "        cnt[stemmer.stem(word.lower())] += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCounter(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, s = 0):\n",
    "        self.s = s\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def html_to_plain_text(self, html):\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.replace_with(\" HYPERLINK \")\n",
    "        return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    def email_to_text(self, email):\n",
    "        html = None\n",
    "        for part in email.walk():\n",
    "            ctype = part.get_content_type()\n",
    "            try:\n",
    "                content = part.get_content()\n",
    "            except:  # in case of encoding issues\n",
    "                content = str(part.get_payload())\n",
    "            if ctype != \"text/html\":\n",
    "                return content\n",
    "            else:\n",
    "                html = content\n",
    "        if html:\n",
    "            return self.html_to_plain_text(html)\n",
    "        \n",
    "\n",
    "    def refine(self, email_text):\n",
    "        url_extractor = urlextract.URLExtract()\n",
    "        urls = url_extractor.find_urls(email_text)#HERE\n",
    "        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "        email_text = re.sub(email_pattern, \"email_address\", email_text)\n",
    "        for url in urls:\n",
    "            email_text = re.sub(re.escape(url), 'url', email_text)\n",
    "        email_text = re.sub(r'\\d+', '', email_text)\n",
    "        email_text = re.sub(r'[^\\w\\s]', '', email_text)\n",
    "        return email_text\n",
    "\n",
    "    def preprocess(self, email_parsed):\n",
    "        email_text = self.email_to_text(email_parsed)\n",
    "        email_text = self.refine(email_text)\n",
    "        cnt = Counter()\n",
    "        stemmer = nltk.PorterStemmer()\n",
    "        for word in email_text.split():\n",
    "            cnt[stemmer.stem(word.lower())] += 1\n",
    "        return cnt\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            self.s += 1\n",
    "            X_transformed.append(self.preprocess(email))\n",
    "        return np.array(X_transformed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'div': 48, 'aligncent': 24, 'a': 15, 'receiv': 12, 'email': 12, 'you': 12, 'of': 12, 'our': 12, 'to': 10, 'or': 10, 'the': 10, 'nbsp': 10, 'thi': 9, 'p': 8, 'one': 8, 'strongfont': 8, 'faceari': 8, 'cartridg': 8, 'are': 6, 'offer': 6, 'in': 6, 'border': 6, 'srcurl': 6, 'width': 6, 'hrefurl': 6, 'at': 6, 'multipart': 4, 'html': 4, 'head': 4, 'bodi': 4, 'aligncenterspan': 4, 'becaus': 4, 'special': 4, 'from': 4, 'market': 4, 'partner': 4, 'if': 4, 'have': 4, 'do': 4, 'not': 4, 'pleas': 4, 'with': 4, 'on': 4, 'price': 4, 'as': 4, 'font': 4, 'img': 4, 'height': 4, 'web': 4, 'site': 4, 'for': 4, 'all': 4, 'best': 4, 'list': 4, 'boundari': 3, 'contenttyp': 2, 'charsetiso': 2, 'contenttransferencod': 2, 'bit': 2, 'stylefontsizeptfontfamilyarialcolorblacky': 2, 'optedin': 2, 'optind': 2, 'through': 2, 'feel': 2, 'error': 2, 'wish': 2, 'addit': 2, 'repli': 2, 'word': 2, 'quotremovequot': 2, 'subject': 2, 'line': 2, 'follow': 2, 'unsubscrib': 2, 'instruct': 2, 'belowop': 2, 'op': 2, 'spanp': 2, 'pa': 2, 'hrefurlimg': 2, 'heighta': 2, 'sizeincred': 2, 'deal': 2, 'inkjet': 2, 'fontstrong': 2, 'sizeour': 2, 'start': 2, 'low': 2, 'strong': 2, 'sizecheck': 2, 'out': 2, 'some': 2, 'dealsfontstrong': 2, 'sizestrongcheck': 2, 'incred': 2, 'dealsstrongfont': 2, 'facearialw': 2, 'been': 2, 'sell': 2, 'onli': 2, 'gener': 2, 'printer': 2, 'over': 2, 'year': 2, 'and': 2, 'come': 2, 'month': 2, 'guaranteefontstrong': 2, 'aligncenternbspfor': 2, 'your': 2, 'shop': 2, 'eudoraautourl': 2, 'urlap': 2, 'stylefontsizeptfontfamilyverdanamsobidifont': 2, 'familytahoma': 2, 'colorblackhow': 2, 'unsubscribebr': 2, 'regist': 2, 'sitesbr': 2, 'sever': 2, 'subscriptionsbr': 2, 'want': 2, 'ani': 2, 'usbr': 2, 'hrefmailtoemail_addresssubjectpleaseremovem': 2, 'target_blankclick': 2, 'hereaspanp': 2, 'is': 1, 'mime': 1, 'messag': 1, 'textplain': 1, 'texthtml': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = PCounter()\n",
    "pc.transform(X_train[[802]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 27 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(pc.transform(X_train[0:3]))\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 77,   3,   4,   2,   2,   1,   1,   3,   1,   1,   1],\n",
       "       [128,   8,   5,   3,   5,   5,   5,   1,   3,   1,   3],\n",
       "       [ 12,   1,   0,   3,   0,   0,   0,   1,   0,   2,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", PCounter()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  1,  3, ...,  0,  0,  0],\n",
       "       [24,  4,  2, ...,  0,  0,  0],\n",
       "       [86, 10, 10, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [11,  3,  0, ...,  0,  0,  0],\n",
       "       [ 6,  2,  0, ...,  0,  0,  0],\n",
       "       [ 4,  0,  4, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.973) total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.959) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.969) total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "reg = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "score = cross_val_score(reg, X_train_transformed, y_train, cv=3, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "reg.fit(X_train_transformed, y_train)\n",
    "y_pred = reg.predict(preprocess_pipeline.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.34%\n",
      "Recall: 99.21%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
